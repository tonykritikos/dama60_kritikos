\documentclass{article}
\usepackage{amsmath}

\begin{document}

\section{Classification}

\subsection{Base Classifiers}
\textbf{Decision Trees}: A tree-like model used to make decisions and predictions based on the values of input features.
\begin{align*}
    \text{Information Gain} &: IG(T, a) = H(T) - \sum_{v \in \text{Values}(a)} \frac{|T_v|}{|T|} H(T_v) \\
    \text{Entropy} &: H(T) = -\sum_{i=1}^{k} p_i \log p_i
\end{align*}

\textbf{Rule-based}: Classifiers that use a set of if-then rules for decision making.

\textbf{Nearest-neighbor}: A type of instance-based learning where the model predicts the label of a new instance based on the most similar instances in the training set.
\begin{align*}
    \text{Distance Metric} &: d(x_i, x_j) = \sqrt{\sum_{k=1}^{n} (x_{ik} - x_{jk})^2}
\end{align*}

\textbf{Naive Bayes}: A probabilistic classifier based on Bayes' theorem with strong (naive) independence assumptions between the features.
\begin{align*}
    \text{Probability} &: P(C_k | x) = \frac{P(C_k) P(x | C_k)}{P(x)}
\end{align*}

\textbf{SVMs (Support Vector Machines)}: Supervised learning models that analyze data for classification and regression analysis, using a hyperplane to separate different classes.
\begin{align*}
    \text{Decision Function} &: f(x) = w^T x + b \\
    \text{Hinge Loss} &: L(y, f(x)) = \max(0, 1 - y f(x))
\end{align*}

\textbf{Neural Networks \& Deep NN}: Computational models inspired by the human brain, composed of layers of interconnected nodes, used for complex pattern recognition.
\begin{align*}
    \text{Perceptron Activation} &: \hat{y} = f(W \cdot x + b) \\
    \text{Sigmoid Function} &: \sigma(x) = \frac{1}{1 + e^{-x}}
\end{align*}

\subsection{Ensemble Classifiers}
\textbf{Boosting}: A method that combines the output of several weak classifiers to create a strong classifier.
\begin{align*}
    \text{AdaBoost Update Rule} &: w_{i+1} = w_i \cdot e^{- \alpha_i y_i h_i(x_i)} \\
    \text{Weight Update} &: \alpha_t = \frac{1}{2} \ln \left( \frac{1 - e_t}{e_t} \right)
\end{align*}

\textbf{Bagging}: Short for Bootstrap Aggregating, it improves the stability and accuracy of machine learning algorithms by combining the predictions of multiple models.
\begin{align*}
    \text{Bootstrap Sample} &: X^* = \{ x_i^* \}
\end{align*}

\textbf{Random Forests}: An ensemble method that uses multiple decision trees to make a prediction, often improving accuracy and controlling overfitting.
\begin{align*}
    \text{Majority Voting} &: \hat{y} = \text{mode}(\hat{y}_1, \hat{y}_2, \ldots, \hat{y}_n)
\end{align*}

\subsection{Keywords for Classification}
\begin{itemize}
    \item \textbf{Test condition}: A condition used to split data in decision trees.
    \item \textbf{Splitting}: The process of dividing data into subsets based on certain conditions or features.
    \item \textbf{Node impurity}: A measure of the homogeneity of a node in a decision tree; lower impurity means more homogeneous nodes.
    \item \textbf{Entropy}: A measure of randomness or disorder, used to calculate information gain in decision trees.
    \item \textbf{Information gain}: The reduction in entropy or impurity after a dataset is split on an attribute.
    \item \textbf{Gain ratio}: A modification of information gain that reduces its bias towards multi-valued attributes.
\end{itemize}

\section{Model Overfitting}

\subsection{Classification Errors}
\begin{itemize}
    \item \textbf{Decision tree leaf nodes}: The final nodes of a decision tree that contain the class label.
    \item \textbf{Overfitting \& underfitting}: Overfitting occurs when a model learns the training data too well, including noise, while underfitting happens when a model is too simple to capture the underlying pattern in the data.
\end{itemize}

\subsection{Model Selection}
\begin{itemize}
    \item \textbf{Using validation set}: A subset of data used to tune model parameters and prevent overfitting.
    \item \textbf{Incorporating model complexity}: Taking into account the complexity of the model when evaluating its performance to avoid overfitting.
    \item \textbf{Pre-pruning (early stopping rule)}: Stopping the growth of a decision tree early to prevent overfitting.
    \item \textbf{Post-pruning}: Removing branches from a fully grown tree to reduce its complexity and improve generalization.
\end{itemize}

\subsection{Regularization Techniques}
\textbf{L1 Regularization (Lasso)}:
\begin{align*}
    \text{Loss Function} &: \text{Loss} = \sum_{i=1}^{n} (y_i - \hat{y_i})^2 + \lambda \sum_{j=1}^{p} |w_j|
\end{align*}

\textbf{L2 Regularization (Ridge)}:
\begin{align*}
    \text{Loss Function} &: \text{Loss} = \sum_{i=1}^{n} (y_i - \hat{y_i})^2 + \lambda \sum_{j=1}^{p} w_j^2
\end{align*}

\subsection{Nearest Neighbor}
\textbf{Cross-validation}: A technique for assessing how the results of a model will generalize to an independent dataset.
\begin{align*}
    \text{k-Fold Cross-Validation} &: \text{CV}_{k} = \frac{1}{k} \sum_{i=1}^{k} \text{error}(M_i)
\end{align*}

\subsection{Metrics for Performance Evaluation}
\textbf{Accuracy}: The ratio of correctly predicted instances to the total instances.
\begin{align*}
    \text{Formula} &: \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\end{align*}

\textbf{Precision}: The ratio of true positive predictions to the total positive predictions.
\begin{align*}
    \text{Formula} &: \text{Precision} = \frac{TP}{TP + FP}
\end{align*}

\textbf{Recall}: The ratio of true positive predictions to the actual positives.
\begin{align*}
    \text{Formula} &: \text{Recall} = \frac{TP}{TP + FN}
\end{align*}

\textbf{ROC (Receiver Operating Characteristic)}: A graphical plot that illustrates the diagnostic ability of a binary classifier system.
\begin{align*}
    \text{ROC Space} &: \text{Plot of } \text{TPR} = \frac{TP}{TP + FN} \text{ vs } \text{FPR} = \frac{FP}{FP + TN}
\end{align*}

\textbf{Error rate}: The ratio of incorrect predictions to the total predictions.
\begin{align*}
    \text{Formula} &: \text{Error Rate} = \frac{FP + FN}{TP + TN + FP + FN}
\end{align*}

\textbf{Specificity}: The ratio of true negative predictions to the actual negatives.

\textbf{FP rate (False Positive rate)}: The ratio of false positives to the total actual negatives.

\textbf{FN rate (False Negative rate)}: The ratio of false negatives to the total actual positives.

\textbf{Power}: The probability that the test correctly rejects a false null hypothesis.

\section{Association Rule Mining}

\subsection{Frequent itemsets}
Sets of items that appear together frequently in a dataset.

\subsection{Support count}
The frequency of occurrence of an itemset in a dataset.

\subsection{Support}
The proportion of transactions in the dataset that contain the itemset.
\begin{align*}
    \text{Formula} &: \text{Support}(A) = \frac{| \{ T | A \subseteq T \} |}{| T |}
\end{align*}

\subsection{Confidence}
The likelihood of occurrence of an itemset given another itemset.
\begin{align*}
    \text{Formula} &: \text{Confidence}(A \Rightarrow B) = \frac{\text{Support}(A \cup B)}{\text{Support}(A)}
\end{align*}

\subsection{Lift}
A measure of the performance of an association rule at predicting the correct outcome compared to a random chance.
\begin{align*}
    \text{Formula} &: \text{Lift}(A \Rightarrow B) = \frac{\text{Support}(A \cup B)}{\text{Support}(A) \times \text{Support}(B)}
\end{align*}

\subsection{Apriori principle}
A rule that states that all non-empty subsets of a frequent itemset must also be frequent.

\subsection{Apriori Algorithm}
\begin{align*}
    \text{Candidate Generation} &: C_{k+1} = \{ X \cup Y \mid X, Y \in L_k, |X \cap Y| = k - 1 \} \\
    \text{Pruning} &: \text{Removing candidates with infrequent subsets.}
\end{align*}

\subsection{FP-Growth}
A frequent pattern mining algorithm that uses a compact data structure called an FP-tree.

\subsection{FP tree}
A tree structure that stores frequent itemsets.

\subsection{Prefix path}
A path in an FP tree that represents the prefix of an itemset.

\section{Cluster Analysis}

\subsection{Partitional Clustering}
\textbf{K-means}: A clustering algorithm that partitions the data into K clusters, each represented by the mean of the points in the cluster.
\begin{align*}
    \text{Centroid Update} &: \mu_k = \frac{1}{|C_k|} \sum_{x_i \in C_k} x_i \\
    \text{Objective Function} &: \sum_{k=1}^{K} \sum_{x_i \in C_k} || x_i - \mu_k ||^2
\end{align*}

\textbf{Elbow Method}: A method to determine the optimal number of clusters by plotting the SSE for different values of \( k \) and selecting the \( k \) at the "elbow" point.

\subsection{Hierarchical Clustering}
\textbf{Agglomerative}: A bottom-up approach where each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.
\begin{align*}
    \text{Single Linkage} &: \text{d}(C_i, C_j) = \min_{x \in C_i, y \in C_j} ||x - y|| \\
    \text{Complete Linkage} &: \text{d}(C_i, C_j) = \max_{x \in C_i, y \in C_j} ||x - y||
\end{align*}

\textbf{Divisive}: A top-down approach where all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.

\textbf{Cluster similarity}: A measure of how similar or dissimilar clusters are.

\textbf{Dendrogram}: A tree-like diagram that records the sequences of merges or splits in hierarchical clustering.

\subsection{DBSCAN}
\textbf{Density-based}: A clustering method based on the density of data points.

\textbf{Density Reachability}: A point \( p \) is directly density-reachable from \( q \) if \( p \) is within \( \epsilon \)-distance of \( q \) and \( q \) has at least MinPts within \( \epsilon \)-distance.

\textbf{Points (border, noise, core)}: Points in DBSCAN are classified as core points, border points, or noise points based on their neighborhood density.

\textbf{Euclidean distance}: A measure of the true straight line distance between two points in Euclidean space.

\subsection{Cluster evaluation}
\textbf{Cohesion}: A measure of how closely related the items in a cluster are.

\textbf{Separation}: A measure of how distinct or well-separated a cluster is from other clusters.

\section{Map Reduce}

\textbf{Apache Spark}: An open-source unified analytics engine for big data processing, with built-in modules for streaming, SQL, machine learning, and graph processing.

\textbf{TensorFlow}: An open-source machine learning framework developed by Google.

\textbf{LSH (Locality-sensitive hashing)}: An algorithmic method of performing probabilistic dimension reduction of high-dimensional data.

\subsection{Hash Functions}
\textbf{Jaccard similarity}: A measure of the similarity between two sets, defined as the size of the intersection divided by the size of the union.
\begin{align*}
    \text{Formula} &: \text{Jaccard similarity}(A, B) = \frac{|A \cap B|}{|A \cup B|}
\end{align*}

\textbf{Hashing}: The process of converting an input into a fixed-size string of bytes, typically for faster data retrieval.

\textbf{Similar items}: Items that have high similarity in a dataset.

\subsection{Shingling}
\textbf{Min-hashing}: A technique used to quickly estimate the similarity of two sets.
\begin{itemize}
    \item \textbf{Signatures}: A compact representation of sets used in min-hashing.
    \item \textbf{Locality-sensitive hashing}: A method for approximate nearest neighbor search in high-dimensional spaces.
\end{itemize}

\textbf{Candidate pairs}: Pairs of items that are likely to be similar, identified using locality-sensitive hashing.

\subsection{Distance Measures}
\textbf{Jaccard distance}: The complement of the Jaccard similarity, measuring dissimilarity between sets.
\begin{align*}
    \text{Formula} &: \text{Jaccard distance}(A, B) = 1 - \text{Jaccard similarity}(A, B)
\end{align*}

\textbf{Bands}: Grouping of hash functions used in locality-sensitive hashing.

\textbf{Hashing Bands}: The process of grouping hash values into bands.

\textbf{S-curve}: A function used to describe the probability of collision in locality-sensitive hashing.

\section{Data Streams}

\subsection{Queries (ad hoc, standing)}
Ad hoc queries are made for a specific task and then discarded, while standing queries are continuously executed over a data stream.

\subsection{Sampling data}
The process of selecting a subset of data from a larger dataset for analysis.

\subsection{Sliding windows}
A method for managing and analyzing data streams where only the most recent data is kept.

\subsection{Filtering}
The process of removing unwanted elements from data streams.

\subsection{Counting distinct elements}
Keeping track of the number of distinct elements in a data stream.

\subsection{Estimating/finding frequent moments}
Identifying frequently occurring items or patterns in data streams.

\subsection{Sliding windows}
\textbf{Bloom filters}: A space-efficient probabilistic data structure used to test whether an element is a member of a set.
\begin{align*}
    \text{Formula} &: \text{P(False Positive)} \approx (1 - e^{-kn/m})^k
\end{align*}

\textbf{Flajolet-Martin algorithm}: An algorithm used for counting the number of distinct elements in a data stream.

\textbf{Count-Min Sketch}:
\begin{align*}
    \text{Frequency Estimation} &: \hat{f}(i) = \min_{j=1}^{d} C[j, h_j(i)]
\end{align*}

\subsection{Link Analysis}
\textbf{Directed / non-directed graphs}: Graphs where edges have a direction or are bidirectional.

\textbf{PageRank}: An algorithm used by Google Search to rank web pages in their search engine results.
\begin{align*}
    \text{PageRank Formula} &: PR(A) = (1-d) + d \left( \sum_{i=1}^{n} \frac{PR(T_i)}{C(T_i)} \right)
\end{align*}
Where \( d \) is the damping factor, \( PR(T_i) \) is the PageRank of linking page \( T_i \), and \( C(T_i) \) is the number of links on page \( T_i \).

\textbf{Random walk}: A mathematical process that describes a path consisting of a succession of random steps.

\textbf{Web problems (dead-ends, spider traps, etc.)}: Issues in web navigation that affect the efficiency of web crawlers and search engines.

\subsection{Page Rank Problems}
\begin{itemize}
    \item \textbf{Measures generic popularity of a page}: PageRank assesses a page's importance based on the number and quality of links to it.
    \item \textbf{Susceptible to link spam}: PageRank can be manipulated through artificial link schemes.
    \item \textbf{Uses a single measure of importance}: PageRank provides a single value to denote a page's importance.
\end{itemize}

\subsection{HITS (Hyperlink-Induced Topic Search)}
\textbf{Hubs and Authorities}: A link analysis algorithm that rates Web pages, focusing on the relationships between hubs and authorities.
\begin{align*}
    \text{Hub Score} &: h(i) = \sum_{j: (i,j) \in E} a(j) \\
    \text{Authority Score} &: a(i) = \sum_{j: (j,i) \in E} h(j)
\end{align*}

\subsection{PCY Algorithm (Park-Chen-Yu)}
\begin{itemize}
    \item \textbf{Buckets}: Hash buckets used to store item pairs.
    \item \textbf{Multistage algorithm}: An algorithm that uses multiple passes to reduce the size of candidate sets.
    \item \textbf{Multihash algorithm}: An algorithm that uses multiple hash functions to reduce the chance of collisions.
    \item \textbf{SON algorithm (Savasere, Omiecinski, and Navathe)}: An algorithm for mining frequent itemsets in large datasets.
\end{itemize}

\section{Distance Metrics}

\textbf{L1 Norm (Manhattan distance)}: The sum of the absolute differences between the coordinates of two points.
\begin{align*}
    \text{Formula} &: ||x||_1 = \sum_{i=1}^{n} |x_i|
\end{align*}

\textbf{L2 Norm (Euclidean distance)}: The straight-line distance between two points in Euclidean space.
\begin{align*}
    \text{Formula} &: ||x||_2 = \left( \sum_{i=1}^{n} x_i^2 \right)^{1/2}
\end{align*}

\textbf{Lâˆž Norm (Maximum norm)}: The maximum absolute difference between the coordinates of two points.
\begin{align*}
    \text{Formula} &: ||x||_\infty = \max_i |x_i|
\end{align*}

\textbf{Mahalanobis Distance}: A measure of the distance between a point and a distribution.
\begin{align*}
    \text{Formula} &: D_M(x, y) = \sqrt{(x - y)^T S^{-1} (x - y)}
\end{align*}

\subsection{Unsupervised Measures}
\textbf{Cohesion}: A measure of how closely related the items in a cluster are.

\textbf{Separation}: A measure of how distinct or well-separated a cluster is from other clusters.

\textbf{Silhouette coefficient}: A measure of how similar an object is to its own cluster compared to other clusters.
\begin{align*}
    \text{Formula} &: s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
\end{align*}
Where \( a(i) \) is the average distance from the \( i \)-th point to the other points in the same cluster, and \( b(i) \) is the minimum average distance from the \( i \)-th point to points in a different cluster.

\subsection{k-means}
\textbf{Centroids}: The center of a cluster in K-means.

\subsection{BFR (BIRCH)}
\begin{itemize}
    \item \textbf{Discard set}: A set of data points discarded as they are outliers or do not fit any cluster well.
    \item \textbf{Compression set}: A set of data points compressed into a few representative points for clustering.
    \item \textbf{Retained set}: A set of data points that are retained for further processing.
\end{itemize}

\section{Online Algorithms}

\subsection{Bipartite Matching}
\textbf{Greedy algorithm}: An algorithm that makes the locally optimal choice at each stage with the hope of finding a global optimum.

\textbf{Competitive ratio}: The ratio between the performance of an online algorithm and an optimal offline algorithm.

\subsection{Web Advertising}
\textbf{AdWords}: An online advertising service developed by Google.
\begin{align*}
    \text{AdWords Auction} &: \text{CPC}_{\text{actual}} = \frac{\text{Ad Rank}_{\text{below}}}{\text{Quality Score}} + \$0.01
\end{align*}

\textbf{Complications (budget, LTR)}: Issues in web advertising related to budget constraints and long-term return.

\textbf{Balance algorithm}: An algorithm to balance different factors in web advertising.

\textbf{Generalized Balance}: An extension of the balance algorithm for more complex scenarios.

\subsection{Recommendation Systems}
\textbf{Content-based recommendation}: A recommendation system that suggests items similar to those a user liked in the past.

\textbf{TF-IDF (Term Frequency-Inverse Document Frequency)}: A statistical measure used to evaluate the importance of a word in a document.

\textbf{Collaborative Filtering}: A method of making recommendations based on the preferences of similar users.

\textbf{User-based collaborative filtering}: A type of collaborative filtering that makes recommendations based on the preferences of similar users.

\textbf{Pearson correlation}: A measure of the linear correlation between two variables.
\begin{align*}
    \text{Formula} &: r = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}}
\end{align*}

\textbf{Cosine similarity}: A measure of similarity between two vectors by calculating the cosine of the angle between them.
\begin{align*}
    \text{Formula} &: \text{cosine\_similarity}(A, B) = \frac{A \cdot B}{||A|| \cdot ||B||}
\end{align*}

\textbf{Item-based CF (rating prediction)}: Collaborative filtering based on the similarities between items.

\section{Dimensionality Reduction}

\textbf{UV-decomposition}: A matrix factorization technique used in dimensionality reduction.

\textbf{SVD (Singular Value Decomposition)}:
\begin{align*}
    A = U \Sigma V^T
\end{align*}

\textbf{Accuracy of recommendation systems}: The degree to which a recommendation system correctly predicts user preferences.

\textbf{Binary classification}: Classification with two possible outcomes.

\textbf{ROC space}: A graphical representation of the performance of a binary classifier.

\textbf{Precision \& recall}: Measures of a model's accuracy in identifying relevant items.

\textbf{nDCG (normalized Discounted Cumulative Gain)}: A measure of ranking quality.

\subsection{Graph Theory}
\textbf{Adjacency matrix}: A matrix representing the connections between nodes in a graph.

\textbf{Adjacency list}: A list representation of the connections between nodes in a graph.

\textbf{Directed/undirected graphs}: Graphs where edges have a direction or are bidirectional.

\textbf{Weighted/unweighted graphs}: Graphs where edges have weights or all edges are equal.

\textbf{Graph walks}: Paths that traverse the vertices and edges of a graph.

\textbf{Random walk}: A mathematical process that describes a path consisting of a succession of random steps.

\textbf{Shortest path}: The path between two nodes that has the smallest total weight.

\textbf{Community detection}: The process of finding groups of nodes in a graph that are more densely connected internally than with the rest of the graph.

\section{Supervised Learning}

\subsection{Linear Model}
\textbf{Linear Regression}: 
\begin{align*}
    y = w^T x + b
\end{align*}

\textbf{Perceptrons}: The simplest type of artificial neural network used for binary classification tasks.

\textbf{Perceptron Update Rule}: 
\begin{align*}
    w \leftarrow w + \Delta w
\end{align*}

\textbf{Winnow algorithm}: A machine learning algorithm that is used for learning linear functions.

\textbf{Perceptron convergence}: The property that the perceptron learning algorithm will converge if the data is linearly separable.

\textbf{SVM Objective}: 
\begin{align*}
    \min_w \frac{1}{2} ||w||^2 + C \sum \max(0, 1 - y_i (w \cdot x_i + b))
\end{align*}

\textbf{Margin}: The distance between the decision boundary and the closest data points from either class.

\textbf{Hinge function}: The loss function used in SVMs that penalizes misclassified points and those within the margin.

\section{Deep Feedforward Networks}

\subsection{Deep Learning}
\textbf{Neural networks}: A set of algorithms modeled loosely after the human brain that are designed to recognize patterns.

\textbf{Input/hidden/output layer}: The layers in a neural network where input data is processed, transformations are applied, and output is produced.

\textbf{Random/pooled/convolutional layer}: Types of layers in a convolutional neural network used for feature extraction and down-sampling.

\subsection{Activation Functions}
\textbf{ReLU (Rectified Linear Unit)}: 
\begin{align*}
    f(x) = \max(0, x)
\end{align*}

\textbf{Sigmoid Function}: 
\begin{align*}
    \sigma(x) = \frac{1}{1 + e^{-x}}
\end{align*}

\textbf{Softmax}: 
\begin{align*}
    \sigma(z)_j = \frac{e^{z_j}}{\sum_{k=1}^{K} e^{z_k}}
\end{align*}

\textbf{Loss functions}: Functions that measure how well the model's predictions match the true data.
\begin{itemize}
    \item \textbf{Regression loss}: Loss function used for regression tasks.
    \item \textbf{Classification loss}: Loss function used for classification tasks.
\end{itemize}

\textbf{Back propagation}: An algorithm for training neural networks by updating weights to minimize the loss function.
\begin{align*}
    \text{Back Propagation} &: \Delta w_{ij} = -\eta \frac{\partial E}{\partial w_{ij}}
\end{align*}
Where \( E \) is the error, \( w_{ij} \) is the weight between nodes \( i \) and \( j \), and \( \eta \) is the learning rate.

\textbf{Gradient Descent}:
\begin{align*}
    \text{Update Rule} &: w \leftarrow w - \eta \nabla_w L(w)
\end{align*}
Where \( \eta \) is the learning rate and \( L(w) \) is the loss function.

\section{Graph Clustering and Analysis}

\textbf{Node degree and Reachability}: Measures of the number of connections a node has and how far it can reach other nodes in the graph.

\textbf{Node similarity (k-core relevance, Affinity, closeness)}: Measures of similarity between nodes based on their connections.

\textbf{Node Proximity}: A measure of how close or far apart nodes are in a graph.

\subsection{Hierarchical Clustering}
\textbf{Betweenness}: A measure of centrality in a graph based on shortest paths.
\begin{align*}
    \text{Formula} &: C_B(v) = \sum_{s \neq v \neq t} \frac{\sigma_{st}(v)}{\sigma_{st}}
\end{align*}
Where \( \sigma_{st} \) is the total number of shortest paths from node \( s \) to node \( t \) and \( \sigma_{st}(v) \) is the number of those paths that pass through \( v \).

\textbf{Girvan-Newman Algorithm}: An algorithm for detecting communities in complex systems by progressively removing edges.

\subsection{Spectral Clustering}
\textbf{Partitioning \& Bipartitioning}: Methods for dividing a graph into clusters or two parts.

\textbf{Graph cuts}: A method of clustering that involves cutting the graph into smaller components.

\textbf{Laplacian matrix}: A matrix representation of a graph that is useful in spectral clustering.
\begin{align*}
    \text{Formula} &: L = D - A
\end{align*}
Where \( D \) is the degree matrix and \( A \) is the adjacency matrix.

\textbf{Eigenvectors and eigenvalues}: Values and vectors that are used to analyze the structure of graphs in spectral clustering.
\begin{align*}
    \text{Eigen Decomposition} &: L \vec{v} = \lambda \vec{v}
\end{align*}

\section{PLA (Perceptron Learning Algorithm)}

\textbf{Variance}: A measure of the spread between numbers in a data set.
\begin{align*}
    \text{Formula} &: \text{Var}(X) = \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2
\end{align*}

\textbf{Covariance}: A measure of the degree to which two variables change together.
\begin{align*}
    \text{Formula} &: \text{Cov}(X, Y) = \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu_X)(y_i - \mu_Y)
\end{align*}

\textbf{Weight Update Rule}: 
\begin{align*}
    w \leftarrow w + \eta (y_i - \hat{y_i}) x_i
\end{align*}
Where \( \eta \) is the learning rate, \( y_i \) is the true label, and \( \hat{y_i} \) is the predicted label.

\end{document}
